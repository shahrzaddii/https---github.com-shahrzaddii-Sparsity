{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Totural on Binary Estimation \n",
    "\n",
    "In this notebook we are going to estimate weights (coefficients) of a linear regression model as binary values. Different scenarios with different methods such as least square, rounding, binary penalty term, and $L_1$ regularisation have been checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario No. 1: A dataset with binary weights\n",
    "\n",
    "In the first scenario a dataset with $n_features=10$ features and $n_samples$ sample has been considered. Dataset is generated over a normal distribution with $\\mu=0$ and $\\sigma^2=1$ or\n",
    "$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "The output is also generated based on a linear function including a noise term as follows \n",
    "\\begin{align*}\n",
    "Y&=X\\beta+\\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "$\\beta$ is a vector of 0 and 1: $\\beta_i \\in \\{1,0\\}, \\beta = \\{\\beta_i | i=1,..,n_{features}\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = np.random.default_rng(seed=1)\n",
    "n_features = 10\n",
    "n_samples = 50\n",
    "var = 1\n",
    "cov = var*np.eye(n_features)\n",
    "mean = np.zeros(n_features)\n",
    "sigma = 1\n",
    "\n",
    "# To define the weights as beta\n",
    "beta = np.array([0, 0, 0, 1, 1, 1, 0, 1, 1, 1])\n",
    "\n",
    "# To define the features/inputs and labels/outputs\n",
    "x = RNG.multivariate_normal(mean, cov, size=n_samples)\n",
    "y = x@beta + RNG.normal(0, sigma**2, size=n_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Square Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leastsquare(x, y):\n",
    "    return np.linalg.solve(x.T.dot(x), x.T.dot(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation algorithm\n",
    "def minimizer(lossfunction, alpha, x, y, n_features):   \n",
    "    initial_beta = np.random.randint(2,size=(n_features))\n",
    "    result = minimize(lossfunction, initial_beta, method='Powell', args=(alpha, x, y, n_features), tol=1e-8)\n",
    "    optimized_beta = result.x\n",
    "    return optimized_beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity: L1 Regularization Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion definition\n",
    "def squared_error(beta, x, y, d):  ## Sum of abs residuals (norm 1)\n",
    "    beta = np.reshape(beta,(-1, d))\n",
    "    return np.sum(abs(beta@x.T-y)**2,axis=1)\n",
    "\n",
    "def L1_lossfunc(beta, alpha, x, y, n_features):\n",
    "    return squared_error(beta, x, y, n_features) + alpha*np.sum(abs(np.reshape(beta,(-1,n_features))),axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity: Binary Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion definition\n",
    "def binary_lossfunc(beta, alpha , x, y, n_features):\n",
    "  pp = []\n",
    "  beta = np.reshape(beta,(-1, n_features))\n",
    "  for i in range((beta.shape)[0]):\n",
    "    p = []\n",
    "    for b in beta[i]:\n",
    "      if b<=0.5:\n",
    "        p.append(alpha*abs(b))\n",
    "      else:\n",
    "        p.append(alpha*abs(b-1))\n",
    "    pp.append(np.sum(p))\n",
    "  return squared_error(beta,x , y, n_features)+np.asarray(pp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Size Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resampling:\n",
    "\n",
    "    def __init__(self, reps, train_size, random_state=None):\n",
    "        self.reps = reps\n",
    "        self.train_size = train_size\n",
    "        self.RNG = np.random.default_rng(random_state)\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.reps\n",
    "\n",
    "    def split(self, x, y=None, groups=None):\n",
    "        for _ in range(self.reps):\n",
    "            train_idx = self.RNG.choice(np.arange(len(x)), size=round(self.train_size*len(x)), replace=False)\n",
    "            test_idx = np.setdiff1d(np.arange(len(x)), train_idx)\n",
    "            np.random.shuffle(test_idx)\n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling_optimizer(reps, x, y, lossfunc, optimizer, alpha):\n",
    "    beta_hat = []\n",
    "    beta_estimation_error_per_repetation = []\n",
    "    std_error_per_samplesize = []\n",
    "    mean_error_per_samplesize = []\n",
    "    repetation = reps\n",
    "    n_samples, n_features = x.shape\n",
    "    if n_samples<10:\n",
    "        training_size = [n_samples]\n",
    "    else:\n",
    "        training_size = np.array(range(1,11))*np.divide(n_samples, 10).astype(np.int16)\n",
    "\n",
    "    for n in training_size:\n",
    "        for r in range(repetation):\n",
    "            sample_index = np.random.choice(len(x), n, replace=False)\n",
    "            sample_size = n\n",
    "            x_input = x[sample_index]\n",
    "            y_label = y[sample_index]\n",
    "            beta_hat.append(optimizer(lossfunc, alpha, x_input, y_label, n_features))\n",
    "            beta_estimation_error_per_repetation.append(np.sum((beta-beta_hat[-1])**2))\n",
    "        mean_error_per_samplesize.append(np.mean(beta_estimation_error_per_repetation))\n",
    "        std_error_per_samplesize.append(np.std(beta_estimation_error_per_repetation))\n",
    "\n",
    "    return np.array(mean_error_per_samplesize), np.array(std_error_per_samplesize), training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition = 100\n",
    "alpha = 1.5\n",
    "l1_beta_hat = []\n",
    "binary_beta_hat = []\n",
    "binary_beta_hat_mean_error = []\n",
    "binary_beta_hat_std_error = []\n",
    "l1_beta_hat_mean_error = []\n",
    "l1_beta_hat_std_error = []\n",
    "training_size = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "\n",
    "for ts in training_size:\n",
    "    res = Resampling(repetition, ts, random_state=0)\n",
    "    splitted_data = res.split(x, y)\n",
    "    for train_idx, test_idx in splitted_data:\n",
    "\n",
    "        l1_beta_hat.append(minimizer(L1_lossfunc, alpha, x[train_idx], y[train_idx], x.shape[1]))\n",
    "        binary_beta_hat.append(minimizer(binary_lossfunc, alpha, x[train_idx], y[train_idx], x.shape[1]))\n",
    "        \n",
    "    l1_beta_hat_mean_error.append(np.mean(np.sum((beta-l1_beta_hat)**2, axis=1)))\n",
    "    l1_beta_hat_std_error.append(np.std(np.sum((beta-l1_beta_hat)**2, axis=1)))\n",
    "    binary_beta_hat_mean_error.append(np.mean(np.sum((beta-binary_beta_hat)**2, axis=1)))\n",
    "    binary_beta_hat_std_error.append(np.std(np.sum((beta-binary_beta_hat)**2, axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 1, figsize=(7, 5))\n",
    "axs.errorbar(np.round(np.array(training_size)*len(x)), binary_beta_hat_mean_error, yerr=np.array(binary_beta_hat_std_error)*1.96/(repetition**0.5), marker='o', color = 'red', label='Binary Penalty')\n",
    "axs.errorbar(np.round(np.array(training_size)*len(x)), l1_beta_hat_mean_error, yerr=np.array(l1_beta_hat_std_error)*1.96/(repetition**0.5), marker='o', color='blue', label='L1 Regul.')\n",
    "axs.set_xlabel('Number of Samples')\n",
    "axs.set_ylabel('$||\\\\hat{\\\\beta}-\\\\beta||_{2}^{2}$')\n",
    "axs.set_title(f'$\\\\alpha$={alpha}, Number of Repetations={repetition}')\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_round(v):\n",
    "    v = np.asarray(v)\n",
    "    bv = []\n",
    "    for x in v:\n",
    "        if abs(x) < abs(x-1):\n",
    "            x = 0\n",
    "        elif abs(x-1) < abs(x):\n",
    "            x = 1\n",
    "        else:\n",
    "            x = x\n",
    "        bv.append(x)\n",
    "    return bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [0.1, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "l1_beta_hat_per_fold, l1_rounded_beta_hat_per_fold, l1_sse_valid_per_fold = [], [], []\n",
    "binary_beta_hat_per_fold,  binary_rounded_beta_hat_per_fold, binary_sse_valid_per_fold = [], [], []\n",
    "\n",
    "l1_mean_sse_valid_per_alpha, l1_beta_hat_mean_error_per_alpha, l1_beta_hat_std_error_per_alpha = [], [], []\n",
    "binary_mean_sse_valid_per_alpha, binary_beta_hat_mean_error_per_alpha, binary_beta_hat_std_error_per_alpha = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=529)\n",
    "for a in alpha_range:\n",
    "    for train_ind, val_ind in kf.split(x,y):\n",
    "\n",
    "        l1_beta_hat_per_fold.append(minimizer(L1_lossfunc, a, x[train_ind], y[train_ind], x.shape[1]))\n",
    "        l1_rounded_beta_hat_per_fold.append(binary_round(l1_beta_hat_per_fold[-1]))\n",
    "        l1_sse_valid_per_fold.append(squared_error(l1_rounded_beta_hat_per_fold[-1], x[val_ind], y[val_ind], n_features))\n",
    "    \n",
    "        binary_beta_hat_per_fold.append(minimizer(binary_lossfunc, a, x[train_ind], y[train_ind], x.shape[1]))\n",
    "        binary_rounded_beta_hat_per_fold.append(binary_round(binary_beta_hat_per_fold[-1]))\n",
    "        binary_sse_valid_per_fold.append(squared_error(binary_rounded_beta_hat_per_fold[-1], x[val_ind], y[val_ind], n_features))\n",
    "        \n",
    "    l1_mean_sse_valid_per_alpha.append(np.mean(l1_sse_valid_per_fold))\n",
    "    l1_beta_hat_mean_error_per_alpha.append(np.mean(np.sum((beta-l1_rounded_beta_hat_per_fold)**2, axis=1)))\n",
    "    l1_beta_hat_std_error_per_alpha.append(np.std(np.sum((beta-l1_rounded_beta_hat_per_fold)**2, axis=1)))\n",
    "    binary_mean_sse_valid_per_alpha.append(np.mean(binary_sse_valid_per_fold))\n",
    "    binary_beta_hat_mean_error_per_alpha.append(np.mean(np.sum((beta-binary_rounded_beta_hat_per_fold)**2, axis=1)))\n",
    "    binary_beta_hat_std_error_per_alpha.append(np.std(np.sum((beta-binary_rounded_beta_hat_per_fold)**2, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axs[0].plot(alpha_range, binary_mean_sse_valid_per_alpha, marker='o', color='red', label='Binary Penalty')\n",
    "axs[0].plot(alpha_range, l1_mean_sse_valid_per_alpha, marker='o', color='blue', label='$L_1$ Reg.')\n",
    "axs[0].set_xlabel('$\\\\alpha$')\n",
    "axs[0].set_ylabel('Mean Squared Error over Validation Data')\n",
    "axs[0].set_title('With Rounding the Weights to 0 & 1')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].errorbar(alpha_range, binary_beta_hat_mean_error_per_alpha, yerr=np.array(binary_beta_hat_std_error_per_alpha)*1.96/(10**0.5), marker='o', color = 'red', label='Binary Penalty')\n",
    "axs[1].errorbar(alpha_range, l1_beta_hat_mean_error_per_alpha, yerr=np.array(l1_beta_hat_std_error_per_alpha)*1.96/(10**0.5), marker='o', color = 'blue', label='$L_1$ Reg.')\n",
    "axs[1].set_xlabel('$\\\\alpha$')\n",
    "axs[1].set_ylabel('$||\\\\hat{\\\\beta}-\\\\beta||_{2}^{2}$')\n",
    "axs[1].set_title('With Rounding the Weights to 0 & 1')\n",
    "axs[1].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario No. 2: Least Square vs. Rounded Least Square for a Binary Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition = 500\n",
    "sample_size = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #percent\n",
    "sample_size = np.round(len(x)*np.array(sample_size)).astype(int)\n",
    "ls_beta_hat_mean_error_per_samplesize, ls_beta_hat_std_error_per_samplesize, ls_sse_train_error_per_samplesize, ls_sse_test_error_per_samplesize = [], [], [], []\n",
    "rls_beta_hat_mean_error_per_samplesize, rls_beta_hat_std_error_per_samplesize, rls_sse_train_error_per_samplesize, rls_sse_test_error_per_samplesize = [], [], [], []\n",
    "ls_std_test_error_per_samplesize, rls_std_test_error_per_samplesize = [], []\n",
    "\n",
    "for ss in sample_size:\n",
    "    sample_id = RNG.choice(np.arange(len(x)), size=ss, replace=False)\n",
    "    x_sample = x[sample_id]\n",
    "    y_sample = y[sample_id]\n",
    "    ls_beta_hat = []\n",
    "    rls_beta_hat = []\n",
    "    ls_sse_train_error = []\n",
    "    ls_sse_test_error = []\n",
    "    rls_sse_train_error = []\n",
    "    rls_sse_test_error = []\n",
    "\n",
    "    res = Resampling(repetition, 0.8, random_state=0)\n",
    "    for train_id, test_id in res.split(x_sample):\n",
    "        ls_beta_hat.append(leastsquare(x_sample[train_id], y_sample[train_id]))\n",
    "        ls_sse_train_error.append(squared_error(ls_beta_hat[-1], x_sample[train_id], y_sample[train_id], x.shape[1])/ss)\n",
    "        ls_sse_test_error.append(squared_error(ls_beta_hat[-1], x_sample[test_id], y_sample[test_id], x.shape[1])/ss)\n",
    "\n",
    "        rls_beta_hat.append(binary_round(ls_beta_hat[-1]))\n",
    "        # rls_sse_train_error.append(squared_error(rls_beta_hat[-1], x_sample[train_id], y_sample[train_id], x.shape[1])/ss)\n",
    "        rls_sse_test_error.append(squared_error(rls_beta_hat[-1], x_sample[test_id], y_sample[test_id], x.shape[1])/ss)\n",
    "\n",
    "\n",
    "\n",
    "    ls_beta_hat_mean_error_per_samplesize.append(np.mean(np.sum((beta-ls_beta_hat)**2, axis=1)))\n",
    "    ls_beta_hat_std_error_per_samplesize.append(np.std(np.sum((beta-ls_beta_hat)**2, axis=1)))\n",
    "    ls_sse_train_error_per_samplesize.append(np.mean(ls_sse_train_error))\n",
    "    ls_sse_test_error_per_samplesize.append(np.mean(ls_sse_test_error))\n",
    "    ls_std_test_error_per_samplesize.append(np.std(ls_sse_test_error))\n",
    "\n",
    "    rls_beta_hat_mean_error_per_samplesize.append(np.mean(np.sum((beta-rls_beta_hat)**2, axis=1)))\n",
    "    rls_beta_hat_std_error_per_samplesize.append(np.std(np.sum((beta-rls_beta_hat)**2, axis=1)))\n",
    "    # rls_sse_train_error_per_samplesize.append(np.mean(rls_sse_train_error))\n",
    "    rls_sse_test_error_per_samplesize.append(np.mean(rls_sse_test_error))\n",
    "    rls_std_test_error_per_samplesize.append(np.std(ls_sse_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "pn=1\n",
    "axs[0].errorbar(sample_size[pn:], ls_beta_hat_mean_error_per_samplesize[pn:], yerr=np.array(ls_beta_hat_std_error_per_samplesize[pn:])*1.96/(repetition**0.5), marker='o', color = 'red', label='Least Squared')\n",
    "axs[0].errorbar(sample_size[pn:], rls_beta_hat_mean_error_per_samplesize[pn:], yerr=np.array(rls_beta_hat_std_error_per_samplesize[pn:])*1.96/(repetition**0.5), marker='o', color='blue', label='Rounded Least Squared')\n",
    "axs[0].set_xlabel('Number of Samples')\n",
    "axs[0].set_ylabel('$||\\\\hat{\\\\beta}-\\\\beta||_{2}^{2}$')\n",
    "axs[0].set_title(f'Number of Repetitions={repetition}')\n",
    "axs[0].legend()\n",
    "\n",
    "# axs[1].plot(sample_size, ls_sse_test_error_per_samplesize, marker='o', color = 'red', label='Least Squared')\n",
    "# axs[1].plot(sample_size, rls_sse_test_error_per_samplesize, marker='o', color='blue', label='Rounded Least Squared')\n",
    "axs[1].errorbar(sample_size, ls_sse_test_error_per_samplesize, yerr=np.array(ls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color = 'red', label='Least Squared')\n",
    "axs[1].errorbar(sample_size, rls_sse_test_error_per_samplesize,yerr=np.array(rls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color='blue', label='Rounded Least Squared')\n",
    "axs[1].set_xlabel('Number of Samples')\n",
    "axs[1].set_ylabel('Sum Squared Error over Test Data')\n",
    "axs[1].set_title(f'Number of Repetitions={repetition}')\n",
    "axs[1].legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario No. 3: A dataset with non-binary weights\n",
    "\n",
    "In this scenario, the database is generated same as before with $n_features=10$ from a normal distribution $X \\sim \\mathcal{N}(\\mu_1=0, \\sigma_1^2=1)$ but $\\beta$ is a vector of non-binary values from $\\beta \\sim \\mathcal{N}(\\mu_2=0, \\sigma_2^2=2)$. Now we are going to find the best binary approximation for $\\beta$ that minimizes the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "n_samples = 100\n",
    "var = 1\n",
    "cov = var*np.eye(n_features)\n",
    "mean = np.zeros(n_features)\n",
    "sigma = 1\n",
    "\n",
    "# To define the weights as beta\n",
    "beta = np.squeeze(RNG.multivariate_normal(np.zeros(1), 2*np.eye(1), size=n_features))\n",
    "\n",
    "# To define the features/inputs and labels/outputs\n",
    "x = RNG.multivariate_normal(mean, cov, size=n_samples)\n",
    "y = x@beta + RNG.normal(0, sigma**2, size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition = 1000\n",
    "sample_size = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #percent\n",
    "sample_size = np.round(len(x)*np.array(sample_size)).astype(int)\n",
    "ls_beta_hat_mean_error_per_samplesize, ls_beta_hat_std_error_per_samplesize, ls_sse_train_error_per_samplesize, ls_sse_test_error_per_samplesize = [], [], [], []\n",
    "rls_beta_hat_mean_error_per_samplesize, rls_beta_hat_std_error_per_samplesize, rls_sse_train_error_per_samplesize, rls_sse_test_error_per_samplesize = [], [], [], []\n",
    "ls_std_train_error_per_samplesize, ls_std_test_error_per_samplesize = [], []\n",
    "rls_std_test_error_per_samplesize, rls_std_train_error_per_samplesize = [], []\n",
    "for ss in sample_size:\n",
    "    sample_id = RNG.choice(np.arange(len(x)), size=ss, replace=False)\n",
    "    x_sample = x[sample_id]\n",
    "    y_sample = y[sample_id]\n",
    "    ls_beta_hat = []\n",
    "    rls_beta_hat = []\n",
    "    ls_sse_train_error = []\n",
    "    ls_sse_test_error = []\n",
    "    rls_sse_train_error = []\n",
    "    rls_sse_test_error = []\n",
    "\n",
    "    res = Resampling(repetition, 0.8, random_state=0)\n",
    "    for train_id, test_id in res.split(x_sample):\n",
    "        ls_beta_hat.append(leastsquare(x_sample[train_id], y_sample[train_id]))\n",
    "        ls_sse_train_error.append(squared_error(ls_beta_hat[-1], x_sample[train_id], y_sample[train_id], x.shape[1]))\n",
    "        ls_sse_test_error.append(squared_error(ls_beta_hat[-1], x_sample[test_id], y_sample[test_id], x.shape[1]))\n",
    "\n",
    "        rls_beta_hat.append(binary_round(ls_beta_hat[-1]))\n",
    "        rls_sse_train_error.append(squared_error(rls_beta_hat[-1], x_sample[train_id], y_sample[train_id], x.shape[1]))\n",
    "        rls_sse_test_error.append(squared_error(rls_beta_hat[-1], x_sample[test_id], y_sample[test_id], x.shape[1]))\n",
    "\n",
    "\n",
    "    ls_beta_hat_mean_error_per_samplesize.append(np.mean(np.sum((beta-ls_beta_hat)**2, axis=1)))\n",
    "    ls_beta_hat_std_error_per_samplesize.append(np.std(np.sum((beta-ls_beta_hat)**2, axis=1)))\n",
    "    ls_sse_train_error_per_samplesize.append(np.mean(ls_sse_train_error)/ss)\n",
    "    ls_std_train_error_per_samplesize.append(np.std(ls_sse_train_error))\n",
    "    ls_sse_test_error_per_samplesize.append(np.mean(ls_sse_test_error)/ss)\n",
    "    ls_std_test_error_per_samplesize.append(np.std(ls_sse_test_error))\n",
    "\n",
    "    rls_beta_hat_mean_error_per_samplesize.append(np.mean(np.sum((beta-rls_beta_hat)**2, axis=1)))\n",
    "    rls_beta_hat_std_error_per_samplesize.append(np.std(np.sum((beta-rls_beta_hat)**2, axis=1)))\n",
    "    rls_sse_train_error_per_samplesize.append(np.mean(rls_sse_train_error)/ss)\n",
    "    rls_std_train_error_per_samplesize.append(np.std(rls_sse_train_error))\n",
    "    rls_sse_test_error_per_samplesize.append(np.mean(rls_sse_test_error)/ss)\n",
    "    rls_std_test_error_per_samplesize.append(np.std(rls_sse_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2, 2, figsize=(12, 7))\n",
    "\n",
    "axs[0,0].errorbar(sample_size, ls_sse_train_error_per_samplesize, yerr=np.array(ls_std_train_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color = 'red', label='Train')\n",
    "axs[0,0].errorbar(sample_size, ls_sse_test_error_per_samplesize, yerr=np.array(ls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color='blue', label='Test')\n",
    "# axs[0,0].set_xlabel('Number of Samples')\n",
    "axs[0,0].set_ylabel('SSE')\n",
    "axs[0,0].set_title(f'Least Square')\n",
    "axs[0,0].legend()\n",
    "\n",
    "axs[0,1].errorbar(sample_size, rls_sse_train_error_per_samplesize, yerr=np.array(rls_std_train_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color = 'red', label='Train')\n",
    "axs[0,1].errorbar(sample_size, rls_sse_test_error_per_samplesize, yerr=np.array(rls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color='blue', label='Test')\n",
    "# axs[0,1].set_xlabel('Number of Samples')\n",
    "# axs[0,1].set_ylabel('SSE')\n",
    "axs[0,1].set_title(f'Rounded Least Square')\n",
    "axs[0,1].legend()\n",
    "\n",
    "axs[1,0].errorbar(sample_size, ls_sse_train_error_per_samplesize, yerr=np.array(ls_std_train_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color = 'blue', label='Least Square')\n",
    "axs[1,0].errorbar(sample_size, rls_sse_train_error_per_samplesize, yerr=np.array(rls_std_train_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color = 'red', label='Rounded Least Square')\n",
    "axs[1,0].set_xlabel('Number of Samples')\n",
    "axs[1,0].set_ylabel('SSE')\n",
    "axs[1,0].set_title(f'Train Dataset')\n",
    "axs[1,0].legend()\n",
    "\n",
    "axs[1,1].errorbar(sample_size, ls_sse_test_error_per_samplesize, yerr=np.array(ls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color='blue', label='Least Square')\n",
    "axs[1,1].errorbar(sample_size, rls_sse_test_error_per_samplesize, yerr=np.array(rls_std_test_error_per_samplesize)*1.96/(repetition**0.5), marker='o', color='red', label='Rounded Least Square')\n",
    "axs[1,1].set_xlabel('Number of Samples')\n",
    "# axs[1,1].set_ylabel('SSE')\n",
    "axs[1,1].set_title(f'Test Dataset')\n",
    "axs[1,1].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
