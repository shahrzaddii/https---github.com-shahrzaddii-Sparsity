{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from binaryapproximator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octet = pd.read_csv('octet.csv')\n",
    "octet_selected = octet[['sign(Delta E)', 'Delta E', 'IP(A)', 'EA(A)', 'H(A)', 'L(A)', 'rs(A)', 'rp(A)', 'rd(A)', 'IP(B)', 'EA(B)', 'H(B)', 'L(B)', 'rs(B)', 'rp(B)', 'rd(B)', 'EN(A)', 'EN(B)']]\n",
    "X_r = octet_selected[['IP(A)', 'EA(A)', 'H(A)', 'L(A)', 'rs(A)', 'rp(A)', 'rd(A)', 'IP(B)', 'EA(B)', 'H(B)', 'L(B)', 'rs(B)', 'rp(B)', 'rd(B)', 'EN(A)', 'EN(B)']].to_numpy()\n",
    "y = octet_selected[['Delta E']].to_numpy()\n",
    "# y = octet_selected[['sign(Delta E)']].to_numpy()\n",
    "# y = np.where( y>=0, np.float32(1), np.float32(0))\n",
    "X,_,_ = normalise_data(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 16\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "# hidden3 = 32\n",
    "output = 1\n",
    "loss_criterion = nn.MSELoss()\n",
    "loo_cv = LeaveOneOut()\n",
    "# dataset = ToTensor(X,y)\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=int(len(X)), shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 0: Regular NN Training\n",
    "1. Initialise the Weights\n",
    "    \n",
    "Do for $t < \\tau$:\n",
    "\n",
    "2. $y_{pred} = Model(X,W_t, \\theta_t)$ (Froward Step)\n",
    "3. $loss(y, y_{pred})$ \n",
    "4. $\\nabla_{w_t} = \\frac{\\partial loss}{\\partial W_t}$ and $\\nabla_{\\theta_t} = \\frac{\\partial loss}{\\partial \\theta_t}$ (Backward Step)\n",
    "5. $W_{t+1} = W_t + \\eta \\nabla_{w_t}$ and $\\theta_{t+1} = \\theta_t + \\eta \\nabla_{\\theta_t}$ (Update Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "model_finals_0 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_0 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_0 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_0_std = np.empty(shape= (82), dtype=object)\n",
    "train_loss_0 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_0_std = np.empty(shape= (10), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_base = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=False)\n",
    "        tr = TrainNN(model=model_base, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        # model_finals_0[reps, set_ind] = model_final\n",
    "        epoch_losses_0[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_0[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_0[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_0_std[samp] = test_loss_0[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_0_std[reps] = train_loss_0[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "fig.suptitle(' Scenario 0: Base NN ', fontsize=15)\n",
    "ax.errorbar(y, test_loss_0.mean(axis=0), yerr=np.array(test_loss_0_std)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test')\n",
    "ax.set_xlabel(\"$\\Delta$E\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(f'Mean and Std of MSE per Test Sample over {n_reps} Repetitions', fontsize=10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dict = dict()\n",
    "for name, param in model_base.named_parameters():\n",
    "    base_dict[name] = param.detach().numpy().flatten()\n",
    "\n",
    "base_df_0 = pd.DataFrame.from_dict(base_dict, orient='index')\n",
    "base_df_0.to_csv('base_model_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Adding L1 and Bianry Penalty + Binarising the Proposition Layer After Full Training\n",
    "1. Initialise the Weights\n",
    "    \n",
    "Do for $t < \\tau$:\n",
    "\n",
    "2. $y_{pred} = Model(X,W_t, \\theta_t)$ (Froward Step)\n",
    "3. $loss(y, y_{pred}, p_{l_1}(\\theta_t), p_{binary}(W_t))$ \n",
    "4. $\\nabla_{w_t} = \\frac{\\partial loss}{\\partial W_t}$ and $\\nabla_{\\theta_t} = \\frac{\\partial loss}{\\partial \\theta_t}$ (Backward Step)\n",
    "5. $W_{t+1} = W_t + \\eta \\nabla_{w_t}$ and $\\theta_{t+1} = \\theta_t + \\eta \\nabla_{\\theta_t}$ (Update Step)\n",
    "\n",
    "After converging\n",
    "\n",
    "6. $W = Binarise(W_{\\tau})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for l1 and binary lambda\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=529)\n",
    "lambda_range = np.logspace(-3, -1, num=10)\n",
    "valid_loss_per_pair_lambda_stp = np.zeros([len(lambda_range), len(lambda_range)])\n",
    "valid_loss_per_pair_lambda_sig = np.zeros([len(lambda_range), len(lambda_range)])\n",
    "for l1_ind, l1 in enumerate(lambda_range):\n",
    "    for lb_ind, lb in enumerate(lambda_range):\n",
    "        valid_loss_stp = []\n",
    "        valid_loss_sig = []\n",
    "        for train_ind, valid_ind in kf.split(X):\n",
    "            model_ = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "            tr = TrainNN(model=model_, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "            loss_func = LossFunction(mse=True, l1_lambda=l1, binary_lambda=lb)\n",
    "            model_final, _ = tr.nn_training(loss_func=loss_func)\n",
    "            model_final.binarise_model()\n",
    "            y_pred_test = model_final.predict_binary(X[valid_ind])\n",
    "            valid_loss_stp.append(np.mean((y[valid_ind]-y_pred_test)**2))\n",
    "            y_pred_test = model_final.predict(X[valid_ind])\n",
    "            valid_loss_sig.append(np.mean((y[valid_ind]-y_pred_test)**2))\n",
    "        valid_loss_per_pair_lambda_stp[l1_ind, lb_ind] = np.mean(valid_loss_stp)\n",
    "        valid_loss_per_pair_lambda_sig[l1_ind, lb_ind] = np.mean(valid_loss_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda_stp_1 = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_stp), valid_loss_per_pair_lambda_stp.shape)[0]]\n",
    "binary_lambda_stp_1 = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_stp), valid_loss_per_pair_lambda_stp.shape)[1]]\n",
    "n_reps = 10\n",
    "model_finals_1 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_1_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_1_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_1_std_stp = np.empty(shape= (82), dtype=object)\n",
    "train_loss_1_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_1_std_stp = np.empty(shape= (10), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_1 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "        tr = TrainNN(model=model_1, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l1_lambda_stp_1, binary_lambda=binary_lambda_stp_1)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        model_final.binarise_model()\n",
    "        # model_finals_0[reps, set_ind] = model_final\n",
    "        epoch_losses_1_stp[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_1_stp[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_1_stp[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_1_std_stp[samp] = test_loss_1_stp[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_1_std_stp[reps] = train_loss_1_stp[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda_sig_1 = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_sig), valid_loss_per_pair_lambda_sig.shape)[0]]\n",
    "binary_lambda_sig_1 = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_sig), valid_loss_per_pair_lambda_sig.shape)[1]]\n",
    "n_reps = 10\n",
    "model_finals_1 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_1_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_1_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_1_std_sig = np.empty(shape= (82), dtype=object)\n",
    "train_loss_1_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_1_std_sig = np.empty(shape= (10), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_1 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "        tr = TrainNN(model=model_1, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l1_lambda_sig_1, binary_lambda=binary_lambda_sig_1)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        model_final.binarise_model()\n",
    "        # model_finals_0[reps, set_ind] = model_final\n",
    "        epoch_losses_1_sig[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_1_sig[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_1_sig[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_1_std_sig[samp] = test_loss_1_sig[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_1_std_sig[reps] = train_loss_1_sig[reps,:].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Binarising the Proposition Layer During Training\n",
    "1- Initialise the Weights (Intentionally we choose random integers from -1 to 1 for hidden layer)\n",
    "\n",
    "Do for $t < \\tau$:\n",
    "\n",
    "2. Keeping $W_t, \\theta_t$ \n",
    "3. $W_b = Binarise(W_t)$ \n",
    "4. $y_{pred} = Model(X,W_b, \\theta_t)$ (Froward Step)\n",
    "5. $loss(y, y_{pred})$ \n",
    "6. $\\nabla_{w_b} = \\frac{\\partial loss}{\\partial W_b}$ and $\\nabla_{\\theta_t} = \\frac{\\partial loss}{\\partial \\theta_t}$ (Backward Step)\n",
    "7. $W_{t+1} = W_t + \\eta \\nabla_{w_b}$ and $\\theta_{t+1} = \\theta_t + \\eta \\nabla_{\\theta_t}$ (Update Step)\n",
    "\n",
    "After converging\n",
    "\n",
    "8. $W = Binarise(W_{\\tau})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reps = 10\n",
    "model_finals_2 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_2 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_2_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_2_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_2_std_stp = np.empty(shape= (82), dtype=object)\n",
    "test_loss_2_std_sig = np.empty(shape= (82), dtype=object)\n",
    "train_loss_2_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_2_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_2_std_stp = np.empty(shape= (n_reps), dtype=object)\n",
    "train_loss_2_std_sig = np.empty(shape= (n_reps), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_2 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "        tr = BinaryTrainNN(model=model_2, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        model_final.binarise_model()\n",
    "        # model_finals_0[reps, set_ind] = model_final\n",
    "        epoch_losses_2[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict_binary(X[test_ind])\n",
    "        test_loss_2_stp[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict_binary(X[train_ind])\n",
    "        train_loss_2_stp[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_2_sig[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_2_sig[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_2_std_stp[samp] = test_loss_2_stp[:,samp].std()\n",
    "    test_loss_2_std_sig[samp] = test_loss_2_sig[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_2_std_stp[reps] = train_loss_2_stp[reps,:].std()\n",
    "    train_loss_2_std_sig[reps] = train_loss_2_sig[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(' Scenario 2 ', fontsize=15)\n",
    "ax[0].errorbar(y, test_loss_2_stp.mean(axis=0), yerr=np.array(test_loss_2_std_stp)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test (step activation)')\n",
    "ax[0].errorbar(y, test_loss_2_sig.mean(axis=0), yerr=np.array(test_loss_2_std_sig)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test (sigmoid activation)')\n",
    "# ax[0].scatter(range(82), train_loss_0.mean(axis=0), label='Train')\n",
    "ax[0].set_xlabel(\"$\\Delta$E\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "# ax[0].set_yscale(\"log\")\n",
    "ax[0].set_title(f'Mean and Std of MSE for Test Set over {n_reps} Repetitions', fontsize=10)\n",
    "ax[0].legend()\n",
    "ax[1].errorbar(range(n_reps), train_loss_2_stp.mean(axis=1), yerr=np.array(train_loss_2_std_stp)*1.96/(n_reps**0.5), marker='o', linestyle='--', label='Train (step activation)')\n",
    "ax[1].errorbar(range(n_reps), train_loss_2_sig.mean(axis=1), yerr=np.array(train_loss_2_std_sig)*1.96/(n_reps**0.5), marker='o', linestyle='--', label='Train (sigmoid activation)')\n",
    "ax[1].set_xlabel(\"Repetition No.\")\n",
    "# ax[1].set_yscale(\"log\")\n",
    "ax[1].set_ylabel(\"loss\")\n",
    "ax[1].set_title(f'Mean and Std of MSE for Train Set over {n_reps} Repetitions', fontsize=10)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dict = dict()\n",
    "for name, param in model_2.named_parameters():\n",
    "    binary_dict[name] = param.detach().numpy().flatten()\n",
    "\n",
    "binary_df_2 = pd.DataFrame.from_dict(binary_dict, orient='index')\n",
    "binary_df_2.to_csv('binary_model_2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Sparsified the Base NN (with adding L1 Penalty)\n",
    "1. Initialise the Weights\n",
    "    \n",
    "Do for $t < \\tau$:\n",
    "\n",
    "2. $y_{pred} = Model(X,W_t, \\theta_t)$ (Froward Step)\n",
    "3. $loss(y, y_{pred}, p_{l_1}(\\theta_t))$  \n",
    "4. $\\nabla_{w_t} = \\frac{\\partial loss}{\\partial W_t}$ and $\\nabla_{\\theta_t} = \\frac{\\partial loss}{\\partial \\theta_t}$ (Backward Step)\n",
    "5. $W_{t+1} = W_t + \\eta \\nabla_{w_t}$ and $\\theta_{t+1} = \\theta_t + \\eta \\nabla_{\\theta_t}$ (Update Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=529)\n",
    "lambda_range = np.logspace(-3, -1, num=10)\n",
    "valid_loss_per_lambda = []\n",
    "for l in lambda_range:\n",
    "    valid_loss_ = []\n",
    "    for train_ind, valid_ind in kf.split(X):\n",
    "        model_ = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=False)\n",
    "        tr = TrainNN(model=model_, x=X[train_ind], y=y[train_ind], gradient_threshold=0.008, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l)\n",
    "        model_final, _ = tr.nn_training(loss_func=loss_func)\n",
    "        y_pred_test = model_final.predict(X[valid_ind])\n",
    "        valid_loss_.append(np.mean((y[valid_ind]-y_pred_test)**2))\n",
    "    valid_loss_per_lambda.append(np.mean(valid_loss_))\n",
    "lambda_range[valid_loss_per_lambda.index(min(valid_loss_per_lambda))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda_3 = lambda_range[valid_loss_per_lambda.index(min(valid_loss_per_lambda))]\n",
    "n_reps = 10\n",
    "model_finals_3 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_3 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_3 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_3_std = np.empty(shape= (82), dtype=object)\n",
    "train_loss_3 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_3_std = np.empty(shape= (n_reps), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_3 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=False)\n",
    "        tr = TrainNN(model=model_3, x=X[train_ind], y=y[train_ind], gradient_threshold=0.008, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l1_lambda_3)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        # model_finals_3[reps, set_ind] = model_final\n",
    "        epoch_losses_3[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_3[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_3[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_3_std[samp] = test_loss_3[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_3_std[reps] = train_loss_3[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(' Scenario 2: Base NN + L1 Penalty ', fontsize=15)\n",
    "ax.errorbar(y, test_loss_3.mean(axis=0), yerr=np.array(test_loss_3_std)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test')\n",
    "# ax.scatter(range(82), train_loss_0.mean(axis=0), label='Train')\n",
    "ax.set_xlabel(\"$\\Delta$E\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_title(f'Mean and Std of MSE for Test Set over {n_reps} Repetitions', fontsize=10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dict = dict()\n",
    "for name, param in model_3.named_parameters():\n",
    "    base_dict[name] = param.detach().numpy().flatten()\n",
    "\n",
    "base_df_3 = pd.DataFrame.from_dict(base_dict, orient='index')\n",
    "base_df_3.to_csv('base_model_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: Adding L1 and Binary Penalty + Binarising the Proposition Layer During Training\n",
    "1- Initialise the Weights (Intentionally we choose random integers from -1 to 1 for hidden layer)\n",
    "\n",
    "Do for $t < \\tau$:\n",
    "\n",
    "2. Keeping $W_t, \\theta_t$ \n",
    "3. $W_b = Binarise(W_t)$ \n",
    "4. $y_{pred} = Model(X,W_b, \\theta_t)$ (Froward Step)\n",
    "5. $loss(y, y_{pred},p_{l_1}(\\theta_t), p_{binary}(W_t))$ \n",
    "6. $\\nabla_{w_b} = \\frac{\\partial loss}{\\partial W_b}$ and $\\nabla_{\\theta_t} = \\frac{\\partial loss}{\\partial \\theta_t}$ (Backward Step)\n",
    "7. $W_{t+1} = W_t + \\eta \\nabla_{w_b}$ and $\\theta_{t+1} = \\theta_t + \\eta \\nabla_{\\theta_t}$ (Update Step)\n",
    "\n",
    "After converging\n",
    "\n",
    "8. $W = Binarise(W_{\\tau})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for l1 and binary lambda\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=529)\n",
    "lambda_range = np.logspace(-3, -1, num=10)\n",
    "valid_loss_per_pair_lambda_stp = np.zeros([len(lambda_range), len(lambda_range)])\n",
    "valid_loss_per_pair_lambda_sig = np.zeros([len(lambda_range), len(lambda_range)])\n",
    "for l1_ind, l1 in enumerate(lambda_range):\n",
    "    for lb_ind, lb in enumerate(lambda_range):\n",
    "        valid_loss_stp = []\n",
    "        valid_loss_sig = []\n",
    "        for train_ind, valid_ind in kf.split(X):\n",
    "            model_ = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "            tr = BinaryTrainNN(model=model_, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "            loss_func = LossFunction(mse=True, l1_lambda=l1, binary_lambda=lb)\n",
    "            model_final, _ = tr.nn_training(loss_func=loss_func)\n",
    "            model_final.binarise_model()\n",
    "            y_pred_test = model_final.predict_binary(X[valid_ind])\n",
    "            valid_loss_stp.append(np.mean((y[valid_ind]-y_pred_test)**2))\n",
    "            y_pred_test = model_final.predict(X[valid_ind])\n",
    "            valid_loss_sig.append(np.mean((y[valid_ind]-y_pred_test)**2))\n",
    "        valid_loss_per_pair_lambda_stp[l1_ind, lb_ind] = np.mean(valid_loss_stp)\n",
    "        valid_loss_per_pair_lambda_sig[l1_ind, lb_ind] = np.mean(valid_loss_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda_4_stp = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_stp), valid_loss_per_pair_lambda_stp.shape)[0]]\n",
    "binary_lambda_4_stp = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_stp), valid_loss_per_pair_lambda_stp.shape)[1]]\n",
    "n_reps = 10\n",
    "model_finals_4 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_4_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_4_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_4_std_stp = np.empty(shape= (82), dtype=object)\n",
    "train_loss_4_stp = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_4_std_stp = np.empty(shape= (n_reps), dtype=object)\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_4 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "        tr = BinaryTrainNN(model=model_4, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l1_lambda_4_stp, binary_lambda=binary_lambda_4_stp)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        model_final.binarise_model()\n",
    "        # model_finals_3[reps, set_ind] = model_final\n",
    "        epoch_losses_4_stp[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict_binary(X[test_ind])\n",
    "        test_loss_4_stp[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict_binary(X[train_ind])\n",
    "        train_loss_4_stp[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_4_std_stp[samp] = test_loss_4_stp[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_4_std_stp[reps] = train_loss_4_stp[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda_4_sig = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_sig), valid_loss_per_pair_lambda_sig.shape)[0]]\n",
    "binary_lambda_4_sig = lambda_range[np.unravel_index(np.argmin(valid_loss_per_pair_lambda_sig), valid_loss_per_pair_lambda_sig.shape)[1]]\n",
    "n_reps = 10\n",
    "model_finals_4 = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "epoch_losses_4_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_4_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "test_loss_4_std_sig = np.empty(shape= (82), dtype=object)\n",
    "train_loss_4_sig = np.empty(shape= (n_reps, 82), dtype=object)\n",
    "train_loss_4_std_sig = np.empty(shape= (n_reps), dtype=object)\n",
    "for reps in range(n_reps):\n",
    "    print(reps)\n",
    "    set_ind = 0\n",
    "    for train_ind, test_ind in loo_cv.split(X):\n",
    "        model_4 = GeneralRegressionNN(input_size=input, hidden_size1=hidden1, hidden_size2=hidden2, output_size=output, balancing_bias=True)\n",
    "        tr = BinaryTrainNN(model=model_4, x=X[train_ind], y=y[train_ind], gradient_threshold=0.005, learning_rate=0.001)\n",
    "        loss_func = LossFunction(mse=True, l1_lambda=l1_lambda_4_sig, binary_lambda=binary_lambda_4_sig)\n",
    "        model_final, epoch_loss = tr.nn_training(loss_func=loss_func)\n",
    "        model_final.binarise_model()\n",
    "        # model_finals_3[reps, set_ind] = model_final\n",
    "        epoch_losses_4_sig[reps, set_ind] = epoch_loss\n",
    "        y_pred_test = model_final.predict(X[test_ind])\n",
    "        test_loss_4_sig[reps, set_ind] = np.mean((y[test_ind]-y_pred_test)**2)\n",
    "        y_pred_train = model_final.predict(X[train_ind])\n",
    "        train_loss_4_sig[reps, set_ind] = np.mean((y[train_ind]-y_pred_train)**2)\n",
    "\n",
    "        set_ind  += 1\n",
    "\n",
    "for samp in range(82):\n",
    "    test_loss_4_std_sig[samp] = test_loss_4_sig[:,samp].std()\n",
    "\n",
    "for reps in range(n_reps):\n",
    "    train_loss_4_std_sig[reps] = train_loss_4_sig[reps,:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(' Scenario 2: Binarised NN + Binary Penalty + L1 Penalty ', fontsize=15)\n",
    "ax[0].errorbar(y, test_loss_4_stp.mean(axis=0), yerr=np.array(test_loss_4_std_stp)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test (step activation)')\n",
    "ax[0].errorbar(y, test_loss_4_sig.mean(axis=0), yerr=np.array(test_loss_4_std_sig)*1.96/(n_reps**0.5), marker='o', linestyle=' ', label='Test (sigmoid activation)')\n",
    "# ax[0].scatter(range(82), train_loss_0.mean(axis=0), label='Train')\n",
    "ax[0].set_xlabel(\"$\\Delta$E\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "# ax[0].set_yscale(\"log\")\n",
    "ax[0].set_title(f'Mean and Std of MSE for Test Set over {n_reps} Repetitions', fontsize=10)\n",
    "ax[0].legend()\n",
    "ax[1].errorbar(range(n_reps), train_loss_4_stp.mean(axis=1), yerr=np.array(train_loss_4_std_stp)*1.96/(n_reps**0.5), marker='o', linestyle='--', label='Train (step activation)')\n",
    "ax[1].errorbar(range(n_reps), train_loss_4_sig.mean(axis=1), yerr=np.array(train_loss_4_std_sig)*1.96/(n_reps**0.5), marker='o', linestyle='--', label='Train (sigmoid activation)')\n",
    "ax[1].set_xlabel(\"Repetition No.\")\n",
    "# ax[1].set_yscale(\"log\")\n",
    "ax[1].set_ylabel(\"loss\")\n",
    "ax[1].set_title(f'Mean and Std of MSE for Train Set over {n_reps} Repetitions', fontsize=10)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dict = dict()\n",
    "for name, param in model_4.named_parameters():\n",
    "    binary_dict[name] = param.detach().numpy().flatten()\n",
    "\n",
    "binary_df_4 = pd.DataFrame.from_dict(binary_dict, orient='index')\n",
    "binary_df_4.to_csv('binary_model_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaxis_test_loss = [test_loss_0,test_loss_2_stp, test_loss_3,test_loss_4_stp, test_loss_2_sig, test_loss_4_sig]\n",
    "yaxis_std_test_loss = [test_loss_0_std,test_loss_2_std_stp, test_loss_3_std,test_loss_4_std_stp,test_loss_2_std_sig, test_loss_4_std_sig]\n",
    "yaxis_train_loss = [train_loss_0,test_loss_2_stp, train_loss_3,train_loss_4_stp, test_loss_2_sig, train_loss_4_sig]\n",
    "yaxis_std_train_loss = [train_loss_0_std,train_loss_2_std_stp, train_loss_3_std,train_loss_4_std_stp, train_loss_2_std_sig, train_loss_4_std_sig]\n",
    "axs_title = ['Base NN', 'Binarised NN (step act.)', 'Base NN + L1 Penalty', 'Binarised NN + L1 & Binary Penalty (step act.)', 'Binarised NN (sigm act.)', 'Binarised NN + L1 & Binary Penalty (sigm act.)']\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "for plot_ind in range(len(yaxis_test_loss)):\n",
    "    ax[0].errorbar(y, yaxis_test_loss[plot_ind].mean(axis=0), yerr=np.array(yaxis_std_test_loss[plot_ind])*1.96/(n_reps**0.5), marker='o', linestyle=' ', label = axs_title[plot_ind])\n",
    "    ax[0].set_xlabel(\"$\\Delta$E\")\n",
    "    ax[0].set_ylabel(\"loss\")\n",
    "    # ax[0].set_yscale(\"log\")\n",
    "    ax[0].set_title(f'Mean and Std of MSE for Test Set over {n_reps} Repetitions', fontsize=10)\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(range(n_reps), np.cumsum(yaxis_train_loss[plot_ind].mean(axis=1)), marker='o', linestyle='--', label=axs_title[plot_ind])\n",
    "    ax[1].set_xlabel(\"Repetition No.\")\n",
    "    # ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_ylabel(\"loss\")\n",
    "    ax[1].set_title(f'Cumulative Mean of MSE for Train Set over {n_reps} Repetitions', fontsize=10)\n",
    "    ax[1].legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
